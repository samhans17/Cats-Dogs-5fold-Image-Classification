{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243864b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "from PIL import ImageFile\n",
    "from keras import layers\n",
    "import keras\n",
    "import warnings\n",
    "import os\n",
    "import shutil\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection as sklrn\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bf88e3",
   "metadata": {},
   "source": [
    "# Setting up the root Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the root directory\n",
    "root_path=\"C:\\\\Users\\\\Moavia computer\\\\Downloads\\\\Compressed\\\\Hashem\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a02d35e",
   "metadata": {},
   "source": [
    "# setting up the directories for the train, test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset name and setting up the directory\n",
    "dataset_folder_name = os.path.join(root_path, 'data')\n",
    "#setting up the class labels\n",
    "class_labels = ['cats', 'dogs']\n",
    "#intiating the empty list for K fold\n",
    "X = []\n",
    "Y = []\n",
    "setting up the paths\n",
    "train_path = os.path.join(dataset_folder_name, 'train_set')\n",
    "test_path = os.path.join(dataset_folder_name, 'test_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4cda78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is to prepare the X List and Y lists so we can use them in K-fold cross validation\n",
    "def prepare_name_with_labels(folder_name, dataset_type='train_set'):\n",
    "    source_files = os.listdir(os.path.join(dataset_folder_name, dataset_type, folder_name))\n",
    "    y_label = \"\"\n",
    "    for i in range(len(class_labels)):\n",
    "        if(folder_name == class_labels[i]):\n",
    "            y_label = class_labels[i]\n",
    "    for val in source_files:\n",
    "        X.append(val)\n",
    "        Y.append(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc6349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize file names and class labels in X and Y variables\n",
    "for i in range(len(class_labels)):\n",
    "    prepare_name_with_labels(class_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As array for the training purposes\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5fd4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X contains names of all the images we have for training\n",
    "print(X)\n",
    "print(len(X))\n",
    "#Y contains the crossponding variables\n",
    "print(Y)\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b1561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is function which takes all the training imagesfrom there folders and add it to the new folder for training and feeding it to the model \n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "def extract_images(folder, destination):\n",
    "    # loop through all the subfolders in the given folder\n",
    "    for subfolder in os.listdir(folder):\n",
    "        subfolder_path = os.path.join(folder, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            # loop through all the files in the subfolder\n",
    "            for file in os.listdir(subfolder_path):\n",
    "                # check if the file is an image\n",
    "                if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "                    file_path = os.path.join(subfolder_path, file)\n",
    "                    # open the image and do something with it\n",
    "                    with Image.open(file_path) as img:\n",
    "                        # create a new file path for the image in the destination folder\n",
    "                        new_file_path = os.path.join(destination, file)\n",
    "                        # copy the image to the destination folder\n",
    "                        shutil.copy(file_path, new_file_path)\n",
    "\n",
    "#setting it up for our directories\n",
    "extract_images(\"data//train_set\", \"data//training_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53add473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the hyperparameter \n",
    "batch_size = 32\n",
    "input_size = (256, 256)\n",
    "NO_OF_EPOCHS = 10\n",
    "NO_OF_FOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8203a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is our model 1\n",
    "def cnn_model1(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential()\n",
    "# Add input layer with input_shape and float32 dtype\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=input_shape, dtype='float32'))\n",
    "    # Add convolutional layer with 32 filters and 3x3 kernel\n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "    # Add another convolutional layer with 64 filters and 3x3 kernel\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))  \n",
    "    # Flatten the output of the convolutional layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    # Add fully connected layer with 512 units and ReLU activation\n",
    "    model.add(tf.keras.layers.Dense(512, activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(128, activation='sigmoid'))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "    # Add output layer with num_classes units and softmax activation\n",
    "    model.add(tf.keras.layers.Dense(units, activation=activation))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b883407",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape_with_channel=(256,256,3)\n",
    "num_classes=2\n",
    "model=cnn_model1(input_shape_with_channel,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc1c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this if you want to see the details of the model.\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba13379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_cross_validate (model, x_data, y_data, n_folds=NO_OF_FOLDS, epochs=NO_OF_EPOCHS, batch_size=batch_size):\n",
    "    # \n",
    "    scores = []\n",
    "    losses=[]\n",
    "    #  we load images using the generators\n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255.,\n",
    "                                       rotation_range=40,\n",
    "                                       width_shift_range=0.2,\n",
    "                                       height_shift_range=0.2,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       horizontal_flip=True)    \n",
    "    validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "          \n",
    "    # prepare cross validation \n",
    "    kfold = sklrn.KFold(n_folds, shuffle=True, random_state=1)\n",
    "    # enumerate splits\n",
    "    FoldsSetNo = 0 \n",
    "    for train_ix, test_ix in kfold.split(x_data):\n",
    "        print ('Folds Set # {0}'.format(FoldsSetNo))\n",
    "        # select rows for train and test\n",
    "        xx_train, yy_train, xx_test, yy_test = \\\n",
    "            x_data[train_ix], y_data[train_ix], x_data[test_ix], y_data[test_ix]\n",
    "\n",
    "        # flow training images in batches for the current folds set\n",
    "        # for training         \n",
    "        train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe = pd.DataFrame({'id':xx_train,'label':yy_train}), \n",
    "            directory=\"data//training_set\", \n",
    "            x_col='id',\n",
    "            y_col='label',\n",
    "            batch_size=batch_size,\n",
    "            target_size=input_size,\n",
    "            class_mode='categorical',\n",
    "            shuffle = False)\n",
    "        \n",
    "        # and for validation         \n",
    "        validation_generator = validation_datagen.flow_from_dataframe(\n",
    "            dataframe = pd.DataFrame({'id':xx_test,'label':yy_test}), \n",
    "            directory=\"data//training_set\", \n",
    "            x_col='id',\n",
    "            y_col='label',\n",
    "            batch_size=batch_size,\n",
    "            target_size=input_size,\n",
    "            class_mode='categorical',\n",
    "            shuffle=False)\n",
    "\n",
    "        # fit the model\n",
    "        history = model.fit(train_generator,\n",
    "                            epochs=epochs,  # The more we train the more our model fits the data\n",
    "                            batch_size=batch_size,  # Smaller batch sizes = samller steps towards convergence\n",
    "                            validation_data=validation_generator,\n",
    "                            verbose=1)\n",
    "        # store scores\n",
    "        scores.append({'acc':np.average(history.history['accuracy']),'val_acc':np.average(history.history['val_accuracy'])})\n",
    "        losses.append({'loss':np.average(history.history['loss']),'val_loss':np.average(history.history['val_loss'])})\n",
    "        FoldsSetNo +=1\n",
    "    return scores, losses\n",
    "print('Starting training and k-fold cross validation ...')\n",
    "scores = train_and_cross_validate(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b6331",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "train = []\n",
    "validation = []\n",
    "plt.subplot(1, 1, 1)\n",
    "for s in scores:\n",
    "    train.append(s['acc'])\n",
    "    validation.append(s['val_acc'])\n",
    "print(train)\n",
    "print(validation)\n",
    "plt.plot(train, color='blue', label='train')\n",
    "plt.plot(validation , color='red', label='validation')\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516eac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses)\n",
    "train = []\n",
    "validation = []\n",
    "plt.subplot(1, 1, 1)\n",
    "for s in losses:\n",
    "    train.append(s['loss'])\n",
    "    validation.append(s['val_loss'])\n",
    "print(train)\n",
    "print(validation)\n",
    "plt.plot(train, color='blue', label='train_loss')\n",
    "plt.plot(validation , color='red', label='validation_loss')\n",
    "plt.title('loss_curves')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'validation_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2456ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is our second model\n",
    "def cnn_model2(input_shape, num_classes):\n",
    "  model = tf.keras.Sequential()\n",
    "  # Add input layer with input_shape and float32 dtype\n",
    "  model.add(tf.keras.layers.InputLayer(input_shape=input_shape, dtype='float32'))\n",
    "  # Add 5 convolutional layers with 32, 64, 128, 256, and 512 filters\n",
    "  model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='sigmoid', input_shape=input_shape))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "  #block2\n",
    "  model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='sigmoid', input_shape=input_shape))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "  #block3  \n",
    "  model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='sigmoid', input_shape=input_shape))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "  #block4\n",
    "  model.add(tf.keras.layers.Conv2D(256, kernel_size=(3, 3), activation='sigmoid', input_shape=input_shape))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "  #block5\n",
    "  model.add(tf.keras.layers.Conv2D(512, kernel_size=(3, 3), activation='sigmoid', input_shape=input_shape))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "  # Add max pooling layer after each convolutional layer\n",
    "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "  \n",
    "  # Flatten the output of the convolutional layers\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  \n",
    "  # Add 2 fully connected layers with 512 and 128 units, respectively\n",
    "  model.add(tf.keras.layers.Dense(512, activation='sigmoid'))\n",
    "  model.add(tf.keras.layers.Dense(128, activation='sigmoid'))\n",
    "    \n",
    "  model.add(layers.Dropout(0.5))\n",
    "  # Add output layer with num_classes units and softmax activation\n",
    "  if num_classes == 2:\n",
    "    activation = \"sigmoid\"\n",
    "    units = 1\n",
    "  else:\n",
    "    activation = \"softmax\"\n",
    "    units = num_classes\n",
    "    \n",
    "  model.add(tf.keras.layers.Dense(units, activation=activation))\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3418efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=cnn_model2(input_shape_with_channel,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6ce1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this if you want to see the details of the model.\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2518bfeb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import sklearn.model_selection as sklrn\n",
    "\n",
    "def train_and_cross_validate (model, x_data, y_data, n_folds=NO_OF_FOLDS, epochs=NO_OF_EPOCHS, batch_size=batch_size):\n",
    "    # \n",
    "    scores = []\n",
    "    losses=[]\n",
    "    #  Loading images through generators ...\n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255.,\n",
    "                                       rotation_range=40,\n",
    "                                       width_shift_range=0.2,\n",
    "                                       height_shift_range=0.2,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       horizontal_flip=True)    \n",
    "    validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "          \n",
    "    # prepare cross validation\n",
    "    kfold = sklrn.KFold(n_folds, shuffle=True, random_state=1)\n",
    "    # enumerate splits\n",
    "    FoldsSetNo = 0 \n",
    "    for train_ix, test_ix in kfold.split(x_data):\n",
    "        print ('Folds Set # {0}'.format(FoldsSetNo))\n",
    "        # select rows for train and test\n",
    "        xx_train, yy_train, xx_test, yy_test = \\\n",
    "            x_data[train_ix], y_data[train_ix], x_data[test_ix], y_data[test_ix]\n",
    "\n",
    "        # flow training images in batches for the current folds set\n",
    "        # for training         \n",
    "        train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe = pd.DataFrame({'id':xx_train,'label':yy_train}), \n",
    "            directory=\"data//training_set\", \n",
    "            x_col='id',\n",
    "            y_col='label',\n",
    "            batch_size=batch_size,\n",
    "            target_size=input_size,\n",
    "            class_mode='categorical',\n",
    "            shuffle = False)\n",
    "        \n",
    "        # and for validation         \n",
    "        validation_generator = validation_datagen.flow_from_dataframe(\n",
    "            dataframe = pd.DataFrame({'id':xx_test,'label':yy_test}), \n",
    "            directory=\"data//training_set\", \n",
    "            x_col='id',\n",
    "            y_col='label',\n",
    "            batch_size=batch_size,\n",
    "            target_size=input_size,\n",
    "            class_mode='categorical',\n",
    "            shuffle=False)\n",
    "\n",
    "        # fit the model\n",
    "        history = model.fit(train_generator,\n",
    "                            epochs=epochs,  # The more we train the more our model fits the data\n",
    "                            batch_size=batch_size,  # Smaller batch sizes = samller steps towards convergence\n",
    "                            validation_data=validation_generator,\n",
    "                            verbose=1)\n",
    "        # store scores\n",
    "        scores.append({'acc':np.average(history.history['accuracy']),'val_acc':np.average(history.history['val_accuracy'])})\n",
    "        losses.append({'loss':np.average(history.history['loss']),'val_loss':np.average(history.history['val_loss'])})\n",
    "        FoldsSetNo +=1\n",
    "    return scores ,losses\n",
    "print('Starting training and k-fold cross validation ...')\n",
    "scores = train_and_cross_validate(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(scores)\n",
    "train = []\n",
    "validation = []\n",
    "plt.subplot(1, 1, 1)\n",
    "for s in scores:\n",
    "    train.append(s['acc'])\n",
    "    validation.append(s['val_acc'])\n",
    "print(train)\n",
    "print(validation)\n",
    "plt.plot(train, color='blue', label='train')\n",
    "plt.plot(validation , color='red', label='validation')\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c254a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses)\n",
    "train = []\n",
    "validation = []\n",
    "plt.subplot(1, 1, 1)\n",
    "for s in losses:\n",
    "    train.append(s['loss'])\n",
    "    validation.append(s['val_loss'])\n",
    "print(train)\n",
    "print(validation)\n",
    "plt.plot(train, color='blue', label='train_loss')\n",
    "plt.plot(validation , color='red', label='validation_loss')\n",
    "plt.title('loss_curves')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'validation_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is our model number 3\n",
    "def cnn_model3(input_shape, num_classes):\n",
    "  model = tf.keras.Sequential()\n",
    "  # Add input layer with input_shape and float32 dtype\n",
    "  model.add(tf.keras.layers.InputLayer(input_shape=input_shape, dtype='float32'))\n",
    "  # Add 5 convolutional layers with 32, 64, 128, 256, and 512 filters\n",
    "  model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "  #block2\n",
    "  model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "  #block3  \n",
    "  model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "  #block4\n",
    "  model.add(tf.keras.layers.Conv2D(256, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "  #block5\n",
    "  model.add(tf.keras.layers.Conv2D(512, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "  # Add max pooling layer after each convolutional layer\n",
    "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "  \n",
    "  # Flatten the output of the convolutional layers\n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "  \n",
    "  # Add 2 fully connected layers with 512 and 128 units, respectively\n",
    "  model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    \n",
    "  model.add(layers.Dropout(0.5))\n",
    "  # Add output layer with num_classes units and softmax activation\n",
    "  if num_classes == 2:\n",
    "    activation = \"sigmoid\"\n",
    "    units = 1\n",
    "  else:\n",
    "    activation = \"softmax\"\n",
    "    units = num_classes\n",
    "    \n",
    "  model.add(tf.keras.layers.Dense(units, activation=activation))\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2392779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=cnn_model3(input_shape_with_channel,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01bdaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model summary  \n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a65c0de",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_and_cross_validate (model, x_data, y_data, n_folds=NO_OF_FOLDS, epochs=NO_OF_EPOCHS, batch_size=batch_size):\n",
    "    # \n",
    "    scores = []\n",
    "    losses = []\n",
    "    #  Loading images through generators ...\n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255.,\n",
    "                                       rotation_range=40,\n",
    "                                       width_shift_range=0.2,\n",
    "                                       height_shift_range=0.2,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       horizontal_flip=True)    \n",
    "    validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "          \n",
    "    # prepare cross validation\n",
    "    kfold = sklrn.KFold(n_folds, shuffle=True, random_state=1)\n",
    "    # enumerate splits\n",
    "    FoldsSetNo = 0 \n",
    "    for train_ix, test_ix in kfold.split(x_data):\n",
    "        print ('Folds Set # {0}'.format(FoldsSetNo))\n",
    "        # select rows for train and test\n",
    "        xx_train, yy_train, xx_test, yy_test = \\\n",
    "            x_data[train_ix], y_data[train_ix], x_data[test_ix], y_data[test_ix]\n",
    "\n",
    "        # flow training images in batches for the current folds set\n",
    "        # for training         \n",
    "        train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe = pd.DataFrame({'id':xx_train,'label':yy_train}), \n",
    "            directory=\"data//training_set\", \n",
    "            x_col='id',\n",
    "            y_col='label',\n",
    "            batch_size=batch_size,\n",
    "            target_size=input_size,\n",
    "            class_mode='binary',\n",
    "            shuffle = False)\n",
    "        \n",
    "        # and for validation         \n",
    "        validation_generator = validation_datagen.flow_from_dataframe(\n",
    "            dataframe = pd.DataFrame({'id':xx_test,'label':yy_test}), \n",
    "            directory=\"data//training_set\", \n",
    "            x_col='id',\n",
    "            y_col='label',\n",
    "            batch_size=batch_size,\n",
    "            target_size=input_size,\n",
    "            class_mode='binary',\n",
    "            shuffle=False)\n",
    "\n",
    "        # fit the model\n",
    "        history = model.fit(train_generator,\n",
    "                            epochs=epochs,  # The more we train the more our model fits the data\n",
    "                            batch_size=batch_size,  # Smaller batch sizes = samller steps towards convergence\n",
    "                            validation_data=validation_generator,\n",
    "                            verbose=1)\n",
    "        # store scores\n",
    "        scores.append({'acc':np.average(history.history['accuracy']),'val_acc':np.average(history.history['val_accuracy'])})\n",
    "        losses.append({'loss':np.average(history.history['loss']),'val_loss':np.average(history.history['val_loss'])})\n",
    "        FoldsSetNo +=1\n",
    "    return scores ,losses\n",
    "print('Starting training and k-fold cross validation ...')\n",
    "scores = train_and_cross_validate(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83536f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# summarize history for accuracy\n",
    "print(scores)\n",
    "train = []\n",
    "validation = []\n",
    "plt.subplot(1, 1, 1)\n",
    "for s in scores:\n",
    "    train.append(s['acc'])\n",
    "    validation.append(s['val_acc'])\n",
    "print(train)\n",
    "print(validation)\n",
    "plt.plot(train, color='blue', label='train')\n",
    "plt.plot(validation , color='red', label='validation')\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95807c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses)\n",
    "train = []\n",
    "validation = []\n",
    "plt.subplot(1, 1, 1)\n",
    "for s in losses:\n",
    "    train.append(s['loss'])\n",
    "    validation.append(s['val_loss'])\n",
    "print(train)\n",
    "print(validation)\n",
    "plt.plot(train, color='blue', label='train_loss')\n",
    "plt.plot(validation , color='red', label='validation_loss')\n",
    "plt.title('loss_curves')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'validation_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce26b4",
   "metadata": {},
   "source": [
    "## this where we import resnet 18 from the keras_resnet.models API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda35ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_resnet.models import ResNet18\n",
    "from keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3b2958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input tensor\n",
    "inputs = Input(shape=(224,224,3))\n",
    "\n",
    "# Initialize the ResNet18 model\n",
    "model = ResNet18(include_top=True, inputs=inputs, classes=1)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_cross_validate (model, x_data, y_data, n_folds=NO_OF_FOLDS, epochs=NO_OF_EPOCHS, batch_size=batch_size):\n",
    "    # \n",
    "    scores = []\n",
    "    \n",
    "    #  Loading images through generators ...\n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255.,\n",
    "                                       rotation_range=40,\n",
    "                                       width_shift_range=0.2,\n",
    "                                       height_shift_range=0.2,\n",
    "                                       shear_range=0.2,\n",
    "                                       zoom_range=0.2,\n",
    "                                       horizontal_flip=True)    \n",
    "    validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "          \n",
    "    # prepare cross validation\n",
    "    kfold = sklrn.KFold(n_folds, shuffle=True, random_state=1)\n",
    "    # enumerate splits\n",
    "    FoldsSetNo = 0 \n",
    "    for train_ix, test_ix in kfold.split(x_data):\n",
    "        print ('Folds Set # {0}'.format(FoldsSetNo))\n",
    "        # select rows for train and test\n",
    "        xx_train, yy_train, xx_test, yy_test = \\\n",
    "            x_data[train_ix], y_data[train_ix], x_data[test_ix], y_data[test_ix]\n",
    "\n",
    "        # flow training images in batches for the current folds set\n",
    "        # for training         \n",
    "        train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe = pd.DataFrame({'id':xx_train,'label':yy_train}), \n",
    "            directory=\"data//training_set\", \n",
    "            x_col='id',\n",
    "            y_col='label',\n",
    "            batch_size=batch_size,\n",
    "            target_size=input_size,\n",
    "            class_mode='binary',\n",
    "            shuffle = False)\n",
    "        \n",
    "        # and for validation         \n",
    "        validation_generator = validation_datagen.flow_from_dataframe(\n",
    "            dataframe = pd.DataFrame({'id':xx_test,'label':yy_test}), \n",
    "            directory=\"data//training_set\", \n",
    "            x_col='id',\n",
    "            y_col='label',\n",
    "            batch_size=batch_size,\n",
    "            target_size=input_size,\n",
    "            class_mode='binary',\n",
    "            shuffle=False)\n",
    "\n",
    "        # fit the model\n",
    "        history = model.fit(train_generator,\n",
    "                            epochs=epochs,  # The more we train the more our model fits the data\n",
    "                            batch_size=batch_size,  # Smaller batch sizes = samller steps towards convergence\n",
    "                            validation_data=validation_generator,\n",
    "                            verbose=1)\n",
    "        # store scores\n",
    "        scores.append({'acc':np.average(history.history['accuracy']),'val_acc':np.average(history.history['val_accuracy'])})\n",
    "        losses.append({'loss':np.average(history.history['loss']),'val_loss':np.average(history.history['val_loss'])})\n",
    "        FoldsSetNo +=1\n",
    "    return scores, losses\n",
    "print('Starting training and k-fold cross validation ...')\n",
    "scores = train_and_cross_validate(model, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da6ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "train = []\n",
    "validation = []\n",
    "plt.subplot(1, 1, 1)\n",
    "for s in scores:\n",
    "    train.append(s['acc'])\n",
    "    validation.append(s['val_acc'])\n",
    "print(train)\n",
    "print(validation)\n",
    "plt.plot(train, color='blue', label='train')\n",
    "plt.plot(validation , color='red', label='validation')\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d76dfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses)\n",
    "train = []\n",
    "validation = []\n",
    "plt.subplot(1, 1, 1)\n",
    "for s in losses:\n",
    "    train.append(s['loss'])\n",
    "    validation.append(s['val_loss'])\n",
    "print(train)\n",
    "print(validation)\n",
    "plt.plot(train, color='blue', label='train_loss')\n",
    "plt.plot(validation , color='red', label='validation_loss')\n",
    "plt.title('loss_curves')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'validation_loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a70c38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e8151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0145f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
